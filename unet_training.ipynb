{
 "cells": [
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "a629980074ebbdaf",
    "outputId": "ba133f68-2e11-4aa2-87b1-8e449e56e387"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'Pytorch-UNet'...\n",
      "remote: Enumerating objects: 618, done.\u001B[K\n",
      "remote: Total 618 (delta 0), reused 0 (delta 0), pack-reused 618 (from 1)\u001B[K\n",
      "Receiving objects: 100% (618/618), 47.42 MiB | 25.95 MiB/s, done.\n",
      "Resolving deltas: 100% (334/334), done.\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "!git clone https://github.com/milesial/Pytorch-UNet"
   ],
   "id": "a629980074ebbdaf"
  },
  {
   "cell_type": "code",
   "source": [
    "%cd Pytorch-UNet"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "YIBSRHrUg580",
    "outputId": "91ce01f1-d3f8-4279-d828-6ce3b9d80163"
   },
   "id": "YIBSRHrUg580",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/Pytorch-UNet\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-29T11:19:28.536702Z",
     "start_time": "2025-06-29T11:19:28.478353Z"
    },
    "id": "initial_id"
   },
   "source": [
    "from unet import UNet\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = UNet(n_channels=8, n_classes=1)\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "-T7mGTZXhIBS",
    "outputId": "52db96db-04bb-4da3-b3fb-1532ff613f80"
   },
   "id": "-T7mGTZXhIBS",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T11:43:08.977363Z",
     "start_time": "2025-06-29T11:43:08.972900Z"
    },
    "id": "4835c4825521684"
   },
   "cell_type": "code",
   "source": [
    "import tifffile\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class TiffDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, scale=1.0):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.masks_dir = Path(masks_dir)\n",
    "        self.scale = scale\n",
    "        self.ids = [f.stem for f in self.images_dir.glob(\"*.tif\")]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.ids[idx]\n",
    "        img = tifffile.imread(str(self.images_dir / f\"{name}.tif\")).astype(np.float32) / 255.0  # [H, W, C]\n",
    "        mask = tifffile.imread(str(self.masks_dir / f\"{name}.tif\")).astype(np.uint8)\n",
    "\n",
    "        if img.ndim == 3:\n",
    "            img = img.transpose(2, 0, 1)  # to [C, H, W]\n",
    "        if mask.ndim == 3:\n",
    "            mask = mask[:, :, 0]  # in case of RGB mask\n",
    "\n",
    "        mask = tifffile.imread(str(self.masks_dir / f\"{name}.tif\")).astype(np.uint8)\n",
    "\n",
    "        # Ensure binary 0/1\n",
    "        if mask.max() > 1:\n",
    "            mask = (mask > 127).astype(np.uint8)  # or use threshold of your choice\n",
    "\n",
    "        mask = torch.from_numpy(mask).float()  # Use float to match loss expectations\n",
    "\n",
    "        return {\n",
    "            'image': torch.from_numpy(img),\n",
    "            'mask': mask  # Already float\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n"
   ],
   "id": "4835c4825521684",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T11:41:20.548619Z",
     "start_time": "2025-06-29T11:41:20.544485Z"
    },
    "id": "fdf41e67251e34e7"
   },
   "cell_type": "code",
   "source": [
    "# Alle Gewichte einfrieren\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Nur die Parameter der letzten Ebene(n) trainierbar machen\n",
    "for param in model.outc.parameters():  # 'outc' ist meist der finale Conv-Layer im UNet\n",
    "    param.requires_grad = True"
   ],
   "id": "fdf41e67251e34e7",
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install importlib"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C9cduft31uXc",
    "outputId": "191e4235-83af-43f8-b02b-2dc9def6dd69"
   },
   "id": "C9cduft31uXc",
   "execution_count": 49,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting importlib\n",
      "  Downloading importlib-1.0.4.zip (7.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Building wheels for collected packages: importlib\n",
      "  Building wheel for importlib (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for importlib: filename=importlib-1.0.4-py3-none-any.whl size=5850 sha256=a4447fbd4da6c18a0910290879138f9b8eb655667a817fb66d63b5d0c04177e1\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/4a/6e/7c4a313549653a504574fa29f907139c752051ef05210df605\n",
      "Successfully built importlib\n",
      "Installing collected packages: importlib\n",
      "Successfully installed importlib-1.0.4\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T11:43:11.479796Z",
     "start_time": "2025-06-29T11:43:11.470470Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "id": "f1186f2257f7ed50",
    "outputId": "b5d80500-fcc5-4550-f1f1-61f01f485582"
   },
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from pathlib import Path\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "from evaluate import evaluate\n",
    "from unet import UNet\n",
    "from utils.data_loading import BasicDataset, CarvanaDataset\n",
    "from utils.dice_score import dice_loss\n",
    "\n",
    "dir_checkpoint = Path('./checkpoints/')\n",
    "\n",
    "import importlib\n",
    "import utils.dice_score\n",
    "importlib.reload(utils.dice_score)\n",
    "import evaluate\n",
    "importlib.reload(evaluate)\n",
    "from evaluate import evaluate\n",
    "from utils.dice_score import dice_loss\n",
    "\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        device,\n",
    "        epochs: int = 5,\n",
    "        batch_size: int = 1,\n",
    "        learning_rate: float = 1e-5,\n",
    "        val_percent: float = 0.1,\n",
    "        save_checkpoint: bool = True,\n",
    "        img_scale: float = 0.5,\n",
    "        amp: bool = False,\n",
    "        weight_decay: float = 1e-8,\n",
    "        momentum: float = 0.999,\n",
    "        gradient_clipping: float = 1.0,\n",
    "):\n",
    "    dataset = TiffDataset('/content/drive/MyDrive/Brick_Data_Train/Brick_Data_Train/Image', '/content/drive/MyDrive/Brick_Data_Train/Brick_Data_Train/Mask')\n",
    "\n",
    "\n",
    "    # 2. Split into train / validation partitions\n",
    "    n_val = int(len(dataset) * val_percent)\n",
    "    n_train = len(dataset) - n_val\n",
    "    train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "    # 3. Create data loaders\n",
    "    loader_args = dict(batch_size=batch_size, num_workers=os.cpu_count(), pin_memory=True)\n",
    "    train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
    "    val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n",
    "\n",
    "    # (Initialize logging)\n",
    "    experiment = wandb.init(project='U-Net', resume='allow', anonymous='must')\n",
    "    experiment.config.update(\n",
    "        dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n",
    "             val_percent=val_percent, save_checkpoint=save_checkpoint, img_scale=img_scale, amp=amp)\n",
    "    )\n",
    "\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {learning_rate}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_checkpoint}\n",
    "        Device:          {device.type}\n",
    "        Images scaling:  {img_scale}\n",
    "        Mixed Precision: {amp}\n",
    "    ''')\n",
    "\n",
    "    # 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
    "    optimizer = optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                              lr=learning_rate, weight_decay=weight_decay, momentum=momentum, foreach=True)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5)  # goal: maximize Dice score\n",
    "    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "    criterion = nn.CrossEntropyLoss() if model.n_classes > 1 else nn.BCEWithLogitsLoss()\n",
    "    global_step = 0\n",
    "\n",
    "    # 5. Begin training\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                images, true_masks = batch['image'], batch['mask']\n",
    "\n",
    "                assert images.shape[1] == model.n_channels, \\\n",
    "                    f'Network has been defined with {model.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n",
    "                true_masks = true_masks.to(device=device, dtype=torch.long)\n",
    "\n",
    "                with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n",
    "                    masks_pred = model(images)\n",
    "                    if model.n_classes == 1:\n",
    "                        loss = criterion(masks_pred.squeeze(1), true_masks.float())\n",
    "                        loss += dice_loss(F.sigmoid(masks_pred.squeeze(1)), true_masks.float(), multiclass=False)\n",
    "                    else:\n",
    "                        loss = criterion(masks_pred, true_masks)\n",
    "                        loss += dice_loss(\n",
    "                            F.softmax(masks_pred, dim=1).float(),\n",
    "                            F.one_hot(true_masks, model.n_classes).permute(0, 3, 1, 2).float(),\n",
    "                            multiclass=True\n",
    "                        )\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                grad_scaler.scale(loss).backward()\n",
    "                grad_scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "                grad_scaler.step(optimizer)\n",
    "                grad_scaler.update()\n",
    "\n",
    "                pbar.update(images.shape[0])\n",
    "                global_step += 1\n",
    "                epoch_loss += loss.item()\n",
    "                experiment.log({\n",
    "                    'train loss': loss.item(),\n",
    "                    'step': global_step,\n",
    "                    'epoch': epoch\n",
    "                })\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                # Evaluation round\n",
    "                division_step = (n_train // (5 * batch_size))\n",
    "                if division_step > 0:\n",
    "                    if global_step % division_step == 0:\n",
    "                        histograms = {}\n",
    "                        for tag, value in model.named_parameters():\n",
    "                            if not value.requires_grad:\n",
    "                                continue  # skip frozen layers\n",
    "                            tag = tag.replace('/', '.')\n",
    "                            if not (torch.isinf(value) | torch.isnan(value)).any():\n",
    "                                histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n",
    "                            if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():\n",
    "                                histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n",
    "\n",
    "                        val_score = evaluate(model, val_loader, device, amp)\n",
    "                        scheduler.step(val_score)\n",
    "\n",
    "                        logging.info('Validation Dice score: {}'.format(val_score))\n",
    "                        try:\n",
    "                            experiment.log({\n",
    "                                'learning rate': optimizer.param_groups[0]['lr'],\n",
    "                                'validation Dice': val_score,\n",
    "                                'images': wandb.Image(images[0].cpu()),\n",
    "                                'masks': {\n",
    "                                    'true': wandb.Image(true_masks[0].float().cpu()),\n",
    "                                    'pred': wandb.Image(masks_pred.argmax(dim=1)[0].float().cpu()),\n",
    "                                },\n",
    "                                'step': global_step,\n",
    "                                'epoch': epoch,\n",
    "                                **histograms\n",
    "                            })\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "        if save_checkpoint:\n",
    "            Path(dir_checkpoint).mkdir(parents=True, exist_ok=True)\n",
    "            state_dict = model.state_dict()\n",
    "            torch.save(state_dict, str(dir_checkpoint / 'checkpoint_epoch{}.pth'.format(epoch)))\n",
    "            logging.info(f'Checkpoint {epoch} saved!')\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks')\n",
    "    parser.add_argument('--epochs', '-e', metavar='E', type=int, default=5, help='Number of epochs')\n",
    "    parser.add_argument('--batch-size', '-b', dest='batch_size', metavar='B', type=int, default=1, help='Batch size')\n",
    "    parser.add_argument('--learning-rate', '-l', metavar='LR', type=float, default=1e-5,\n",
    "                        help='Learning rate', dest='lr')\n",
    "    parser.add_argument('--load', '-f', type=str, default=False, help='Load model from a .pth file')\n",
    "    parser.add_argument('--scale', '-s', type=float, default=0.5, help='Downscaling factor of the images')\n",
    "    parser.add_argument('--validation', '-v', dest='val', type=float, default=10.0,\n",
    "                        help='Percent of the data that is used as validation (0-100)')\n",
    "    parser.add_argument('--amp', action='store_true', default=False, help='Use mixed precision')\n",
    "    parser.add_argument('--bilinear', action='store_true', default=False, help='Use bilinear upsampling')\n",
    "    parser.add_argument('--classes', '-c', type=int, default=2, help='Number of classes')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "train_model(model=model, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
   ],
   "id": "f1186f2257f7ed50",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-56-2602298253.py:75: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
      "Epoch 1/5:  15%|█▌        | 2/13 [00:04<00:25,  2.35s/img, loss (batch)=nan]\n",
      "Validation round:   0%|          | 0/1 [00:00<?, ?batch/s]\u001B[A\n",
      "Validation round: 100%|██████████| 1/1 [00:02<00:00,  2.07s/batch]\u001B[A\n",
      "Epoch 1/5:  31%|███       | 4/13 [00:11<00:23,  2.66s/img, loss (batch)=nan]\n",
      "Validation round:   0%|          | 0/1 [00:00<?, ?batch/s]\u001B[A\n",
      "Validation round: 100%|██████████| 1/1 [00:03<00:00,  3.06s/batch]\u001B[A\n",
      "Epoch 1/5:  46%|████▌     | 6/13 [00:19<00:22,  3.24s/img, loss (batch)=nan]\n",
      "Validation round:   0%|          | 0/1 [00:00<?, ?batch/s]\u001B[A\n",
      "Validation round: 100%|██████████| 1/1 [00:02<00:00,  2.12s/batch]\u001B[A\n",
      "Epoch 1/5:  62%|██████▏   | 8/13 [00:25<00:15,  3.07s/img, loss (batch)=nan]\n",
      "Validation round:   0%|          | 0/1 [00:00<?, ?batch/s]\u001B[A\n",
      "Validation round: 100%|██████████| 1/1 [00:03<00:00,  3.08s/batch]\u001B[A\n",
      "Epoch 1/5:  77%|███████▋  | 10/13 [00:33<00:10,  3.34s/img, loss (batch)=nan]\n",
      "Validation round:   0%|          | 0/1 [00:00<?, ?batch/s]\u001B[A\n",
      "Validation round: 100%|██████████| 1/1 [00:02<00:00,  2.00s/batch]\u001B[A\n",
      "Epoch 1/5:  92%|█████████▏| 12/13 [00:39<00:03,  3.08s/img, loss (batch)=nan]\n",
      "Validation round:   0%|          | 0/1 [00:00<?, ?batch/s]\u001B[A\n",
      "Validation round: 100%|██████████| 1/1 [00:02<00:00,  2.97s/batch]\u001B[A\n",
      "Epoch 1/5: 100%|██████████| 13/13 [00:45<00:00,  3.52s/img, loss (batch)=nan]\n",
      "Epoch 2/5:   8%|▊         | 1/13 [00:02<00:26,  2.20s/img, loss (batch)=nan]\n",
      "Validation round:   0%|          | 0/1 [00:00<?, ?batch/s]\u001B[A\n",
      "Validation round: 100%|██████████| 1/1 [00:01<00:00,  1.93s/batch]\u001B[A\n",
      "Epoch 2/5:  23%|██▎       | 3/13 [00:08<00:27,  2.70s/img, loss (batch)=nan]\n",
      "Validation round:   0%|          | 0/1 [00:00<?, ?batch/s]\u001B[A\n",
      "Validation round: 100%|██████████| 1/1 [00:03<00:00,  3.54s/batch]\u001B[A\n",
      "Epoch 2/5:  38%|███▊      | 5/13 [00:16<00:26,  3.31s/img, loss (batch)=nan]\n",
      "Validation round:   0%|          | 0/1 [00:00<?, ?batch/s]\u001B[A\n",
      "Validation round: 100%|██████████| 1/1 [00:02<00:00,  2.08s/batch]\u001B[A\n",
      "Epoch 2/5:  54%|█████▍    | 7/13 [00:22<00:18,  3.07s/img, loss (batch)=nan]\n",
      "Validation round:   0%|          | 0/1 [00:00<?, ?batch/s]\u001B[A\n",
      "Epoch 2/5:  54%|█████▍    | 7/13 [00:23<00:20,  3.40s/img, loss (batch)=nan]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-56-2602298253.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    164\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mparser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparse_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    165\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 166\u001B[0;31m \u001B[0mtrain_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'cuda'\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_available\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m'cpu'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipython-input-56-2602298253.py\u001B[0m in \u001B[0;36mtrain_model\u001B[0;34m(model, device, epochs, batch_size, learning_rate, val_percent, save_checkpoint, img_scale, amp, weight_decay, momentum, gradient_clipping)\u001B[0m\n\u001B[1;32m    136\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 138\u001B[0;31m                         \u001B[0mval_score\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mevaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mamp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    139\u001B[0m                         \u001B[0mscheduler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mval_score\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    114\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mctx_factory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 116\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    117\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/Pytorch-UNet/evaluate.py\u001B[0m in \u001B[0;36mevaluate\u001B[0;34m(net, dataloader, device, amp)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m             \u001B[0;31m# predict the mask\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m             \u001B[0mmask_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mnet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_classes\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1738\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1739\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1740\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1741\u001B[0m     \u001B[0;31m# torchrec tests the code consistency with the following code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1748\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1749\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1751\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1752\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/Pytorch-UNet/unet/unet_model.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m         \u001B[0mx1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m         \u001B[0mx2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdown1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m         \u001B[0mx3\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdown2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1738\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1739\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1740\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1741\u001B[0m     \u001B[0;31m# torchrec tests the code consistency with the following code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1748\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1749\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1751\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1752\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/Pytorch-UNet/unet/unet_parts.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdouble_conv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1738\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1739\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1740\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1741\u001B[0m     \u001B[0;31m# torchrec tests the code consistency with the following code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1748\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1749\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1751\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1752\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    248\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    249\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 250\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    251\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1738\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1739\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1740\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1741\u001B[0m     \u001B[0;31m# torchrec tests the code consistency with the following code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1748\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1749\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1751\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1752\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    552\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    553\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 554\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conv_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    555\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    556\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001B[0m in \u001B[0;36m_conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    547\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroups\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    548\u001B[0m             )\n\u001B[0;32m--> 549\u001B[0;31m         return F.conv2d(\n\u001B[0m\u001B[1;32m    550\u001B[0m             \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstride\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdilation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroups\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    551\u001B[0m         )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 56
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
