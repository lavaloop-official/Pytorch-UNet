{
  "cells": [
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a629980074ebbdaf",
        "outputId": "ee99ece9-7b31-438e-fd60-1602ad2ec102"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Pytorch-UNet'...\n",
            "remote: Enumerating objects: 618, done.\u001b[K\n",
            "remote: Total 618 (delta 0), reused 0 (delta 0), pack-reused 618 (from 1)\u001b[K\n",
            "Receiving objects: 100% (618/618), 47.42 MiB | 12.97 MiB/s, done.\n",
            "Resolving deltas: 100% (334/334), done.\n"
          ]
        }
      ],
      "execution_count": 1,
      "source": [
        "!git clone https://github.com/milesial/Pytorch-UNet"
      ],
      "id": "a629980074ebbdaf"
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Pytorch-UNet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIBSRHrUg580",
        "outputId": "1457eb77-606e-401f-815c-a4af71309e2f"
      },
      "id": "YIBSRHrUg580",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Pytorch-UNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2025-06-29T11:19:28.536702Z",
          "start_time": "2025-06-29T11:19:28.478353Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_id",
        "outputId": "e96e1dce-5398-43f3-ede3-c5124b3274c2"
      },
      "source": [
        "from unet import UNet\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = UNet(n_channels=8, n_classes=1, bilinear=True)\n",
        "model.to(device)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (inc): DoubleConv(\n",
              "    (double_conv): Sequential(\n",
              "      (0): Conv2d(8, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (down1): Down(\n",
              "    (maxpool_conv): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (1): DoubleConv(\n",
              "        (double_conv): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (down2): Down(\n",
              "    (maxpool_conv): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (1): DoubleConv(\n",
              "        (double_conv): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (down3): Down(\n",
              "    (maxpool_conv): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (1): DoubleConv(\n",
              "        (double_conv): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (down4): Down(\n",
              "    (maxpool_conv): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (1): DoubleConv(\n",
              "        (double_conv): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (up1): Up(\n",
              "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
              "    (conv): DoubleConv(\n",
              "      (double_conv): Sequential(\n",
              "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (up2): Up(\n",
              "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
              "    (conv): DoubleConv(\n",
              "      (double_conv): Sequential(\n",
              "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (up3): Up(\n",
              "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
              "    (conv): DoubleConv(\n",
              "      (double_conv): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (up4): Up(\n",
              "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
              "    (conv): DoubleConv(\n",
              "      (double_conv): Sequential(\n",
              "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (outc): OutConv(\n",
              "    (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T7mGTZXhIBS",
        "outputId": "11b1c4fc-cd9b-449c-a329-75d8a916b117"
      },
      "id": "-T7mGTZXhIBS",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-29T11:43:08.977363Z",
          "start_time": "2025-06-29T11:43:08.972900Z"
        },
        "id": "4835c4825521684"
      },
      "cell_type": "code",
      "source": [
        "import tifffile\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from os import listdir\n",
        "from os.path import splitext, isfile, join\n",
        "import logging\n",
        "from PIL import Image\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F # Import F for padding\n",
        "\n",
        "def load_image(filename):\n",
        "    # This function is not needed for .tif files as tifffile.imread handles loading\n",
        "    pass\n",
        "\n",
        "def unique_mask_values(idx, mask_dir, mask_suffix):\n",
        "    mask_file = list(mask_dir.glob(idx + mask_suffix + '.*'))\n",
        "    if not mask_file:\n",
        "        raise FileNotFoundError(f\"Mask file not found for ID: {idx}\")\n",
        "    mask_file = mask_file[0]\n",
        "    mask = tifffile.imread(str(mask_file)).astype(np.uint8)\n",
        "    if mask.ndim == 3:\n",
        "        mask = mask[:, :, 0] # Take the first channel if it's an RGB mask\n",
        "    return np.unique(mask)\n",
        "\n",
        "\n",
        "class BasicDataset(Dataset):\n",
        "    def __init__(self, images_dir: str, mask_dir: str, scale: float = 1.0, mask_suffix: str = '', pad_to_multiple: int = 32):\n",
        "        self.images_dir = Path(images_dir)\n",
        "        self.mask_dir = Path(mask_dir)\n",
        "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
        "        self.scale = scale\n",
        "        self.mask_suffix = mask_suffix\n",
        "        self.pad_to_multiple = pad_to_multiple # Added padding attribute\n",
        "\n",
        "        self.ids = [splitext(file)[0] for file in listdir(images_dir) if isfile(join(images_dir, file)) and not file.startswith('.')]\n",
        "        if not self.ids:\n",
        "            raise RuntimeError(f'No input file found in {images_dir}, make sure you put your images there')\n",
        "\n",
        "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
        "        logging.info('Scanning mask files to determine unique values')\n",
        "        # Use a smaller pool size or adjust based on your system resources if needed\n",
        "        with Pool(processes=4) as p:\n",
        "             unique = list(tqdm(\n",
        "                 p.imap(partial(unique_mask_values, mask_dir=self.mask_dir, mask_suffix=self.mask_suffix), self.ids),\n",
        "                 total=len(self.ids)\n",
        "             ))\n",
        "\n",
        "\n",
        "        self.mask_values = list(sorted(np.unique(np.concatenate(unique)).tolist()))\n",
        "        logging.info(f'Unique mask values: {self.mask_values}')\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.ids[idx]\n",
        "        img_file = list(self.images_dir.glob(name + '.*'))\n",
        "        mask_file = list(self.mask_dir.glob(name + self.mask_suffix + '.*'))\n",
        "\n",
        "        if not img_file:\n",
        "             raise FileNotFoundError(f\"Image file not found for ID: {name}\")\n",
        "        if not mask_file:\n",
        "             raise FileNotFoundError(f\"Mask file not found for ID: {name}\")\n",
        "\n",
        "        img = tifffile.imread(str(img_file[0])).astype(np.float32) / 255.0  # [H, W, C]\n",
        "        img = np.nan_to_num(img, nan=0.0)\n",
        "\n",
        "        mask = tifffile.imread(str(mask_file[0])).astype(np.uint8)\n",
        "\n",
        "        # Handle potential scaling (optional, based on original BasicDataset)\n",
        "        # if self.scale != 1.0:\n",
        "        #     img = self.scale_image(img, self.scale)\n",
        "        #     mask = self.scale_image(mask, self.scale)\n",
        "\n",
        "        if img.ndim == 3:\n",
        "            img = img.transpose(2, 0, 1)  # to [C, H, W]\n",
        "        elif img.ndim == 2: # Handle grayscale images by adding a channel dimension\n",
        "             img = np.expand_dims(img, axis=0)\n",
        "\n",
        "        if mask.ndim == 3:\n",
        "            mask = mask[:, :, 0]  # in case of RGB mask\n",
        "\n",
        "        # Convert mask to binary (0 and 1)\n",
        "        # Assuming all non-zero values represent the foreground\n",
        "        mask = (mask > 0).astype(np.uint8)\n",
        "\n",
        "        # Convert to tensors before padding\n",
        "        img_tensor = torch.from_numpy(img)\n",
        "        mask_tensor = torch.from_numpy(mask).long()\n",
        "\n",
        "        # Pad images and masks to be divisible by pad_to_multiple\n",
        "        _, h, w = img_tensor.shape\n",
        "        pad_h = (self.pad_to_multiple - (h % self.pad_to_multiple)) % self.pad_to_multiple\n",
        "        pad_w = (self.pad_to_multiple - (w % self.pad_to_multiple)) % self.pad_to_multiple\n",
        "\n",
        "        # Pad image tensor (C, H, W)\n",
        "        img_tensor = F.pad(img_tensor, (0, pad_w, 0, pad_h))\n",
        "\n",
        "        # Pad mask tensor (H, W) - need to add channel dimension for F.pad\n",
        "        mask_tensor = mask_tensor.unsqueeze(0) # Add channel dimension\n",
        "        mask_tensor = F.pad(mask_tensor, (0, pad_w, 0, pad_h))\n",
        "        mask_tensor = mask_tensor.squeeze(0) # Remove channel dimension\n",
        "\n",
        "        return {\n",
        "            'image': img_tensor,\n",
        "            'mask': mask_tensor\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(mask_values, pil_img, scale, is_mask):\n",
        "        # This method is from the original BasicDataset but is not directly used\n",
        "        # with tifffile. It's kept here for reference or if needed for other image types\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def scale_image(img, scale):\n",
        "        # Placeholder for image scaling if needed\n",
        "        # You would need to implement image resizing here (e.g., using OpenCV or PIL)\n",
        "        return img"
      ],
      "id": "4835c4825521684",
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install importlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9cduft31uXc",
        "outputId": "b7aecec0-ede5-4e9e-ad46-12304a6e3c5a"
      },
      "id": "C9cduft31uXc",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting importlib\n",
            "  Downloading importlib-1.0.4.zip (7.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: importlib\n",
            "  Building wheel for importlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for importlib: filename=importlib-1.0.4-py3-none-any.whl size=5850 sha256=4efd2dae34dab960aab0c6c6f82249dfe3e5836017247bbc88a394f6e31889c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/4a/6e/7c4a313549653a504574fa29f907139c752051ef05210df605\n",
            "Successfully built importlib\n",
            "Installing collected packages: importlib\n",
            "Successfully installed importlib-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "from utils.dice_score import multiclass_dice_coeff, dice_coeff\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def evaluate(net, dataloader, device, amp):\n",
        "    net.eval()\n",
        "    num_val_batches = len(dataloader)\n",
        "    dice_score = 0\n",
        "\n",
        "    # iterate over the validation set\n",
        "    with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n",
        "        for batch in tqdm(dataloader, total=num_val_batches, desc='Validation round', unit='batch', leave=False):\n",
        "            image, mask_true = batch['image'], batch['mask']\n",
        "\n",
        "            # move images and labels to correct device and type\n",
        "            image = image.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n",
        "            mask_true = mask_true.to(device=device, dtype=torch.long)\n",
        "\n",
        "            # predict the mask\n",
        "            mask_pred = net(image)\n",
        "\n",
        "            if net.n_classes == 1:\n",
        "                assert mask_true.min() >= 0 and mask_true.max() <= 1, 'True mask indices should be in [0, 1]'\n",
        "                mask_pred = F.sigmoid(mask_pred) # Apply sigmoid here\n",
        "                mask_pred = (mask_pred > 0.5).float()\n",
        "                # Squeeze the predicted mask to remove the channel dimension\n",
        "                mask_pred = mask_pred.squeeze(1)\n",
        "                # compute the Dice score\n",
        "                dice_score += dice_coeff(mask_pred, mask_true, reduce_batch_first=False)\n",
        "            else:\n",
        "                assert mask_true.min() >= 0 and mask_true.max() < net.n_classes, 'True mask indices should be in [0, n_classes['\n",
        "                # convert to one-hot format\n",
        "                mask_true = F.one_hot(mask_true, net.n_classes).permute(0, 3, 1, 2).float()\n",
        "                mask_pred = F.one_hot(mask_pred.argmax(dim=1), net.n_classes).permute(0, 3, 1, 2).float()\n",
        "                # compute the Dice score, ignoring background\n",
        "                dice_score += multiclass_dice_coeff(mask_pred[:, 1:], mask_true[:, 1:], reduce_batch_first=False)\n",
        "\n",
        "    net.train()\n",
        "    return dice_score / max(num_val_batches, 1)"
      ],
      "metadata": {
        "id": "kQ0y2QQF5JG1"
      },
      "id": "kQ0y2QQF5JG1",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-29T11:43:11.479796Z",
          "start_time": "2025-06-29T11:43:11.470470Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f1186f2257f7ed50",
        "outputId": "ed60bf12-802c-4012-b1d9-8e581b43600f"
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from pathlib import Path\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "import wandb\n",
        "from unet import UNet\n",
        "from utils.dice_score import dice_loss\n",
        "\n",
        "dir_checkpoint = Path('./checkpoints/')\n",
        "\n",
        "import importlib\n",
        "import utils.dice_score\n",
        "importlib.reload(utils.dice_score)\n",
        "from utils.dice_score import dice_loss\n",
        "\n",
        "\n",
        "def train_model(\n",
        "        model,\n",
        "        device,\n",
        "        epochs: int = 100,\n",
        "        batch_size: int = 16,\n",
        "        learning_rate: float = 1e-4,\n",
        "        val_percent: float = 0.1,\n",
        "        save_checkpoint: bool = True,\n",
        "        img_scale: float = 1,\n",
        "        amp: bool = True,\n",
        "        weight_decay: float = 1e-5,\n",
        "        momentum: float = 0.9,\n",
        "        gradient_clipping: float = 1.0,\n",
        "):\n",
        "    dataset = BasicDataset('/content/drive/MyDrive/Brick_Data_Train_unzipped/Brick_Data_Train/Image', '/content/drive/MyDrive/Brick_Data_Train_unzipped/Brick_Data_Train/Mask')\n",
        "\n",
        "    # 2. Split into train / validation partitions\n",
        "    n_val = int(len(dataset) * val_percent)\n",
        "    n_train = len(dataset) - n_val\n",
        "    train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
        "\n",
        "    # 3. Create data loaders\n",
        "    loader_args = dict(batch_size=batch_size, num_workers=os.cpu_count(), pin_memory=True)\n",
        "    train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
        "    val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n",
        "\n",
        "    # (Initialize logging)\n",
        "    experiment = wandb.init(project='U-Net', resume='allow', anonymous='must')\n",
        "    experiment.config.update(\n",
        "        dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n",
        "             val_percent=val_percent, save_checkpoint=save_checkpoint, img_scale=img_scale, amp=amp)\n",
        "    )\n",
        "\n",
        "    print(f'''Starting training:\n",
        "        Epochs:          {epochs}\n",
        "        Batch size:      {batch_size}\n",
        "        Learning rate:   {learning_rate}\n",
        "        Training size:   {n_train}\n",
        "        Validation size: {n_val}\n",
        "        Checkpoints:     {save_checkpoint}\n",
        "        Device:          {device.type}\n",
        "        Images scaling:  {img_scale}\n",
        "        Mixed Precision: {amp}\n",
        "    ''')\n",
        "\n",
        "    # 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                              lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5)  # goal: maximize Dice score\n",
        "    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
        "    criterion = nn.BCEWithLogitsLoss() # Use BCEWithLogitsLoss for binary segmentation\n",
        "    global_step = 0\n",
        "\n",
        "    # 5. Begin training\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:\n",
        "            for batch in train_loader:\n",
        "                images, true_masks = batch['image'], batch['mask']\n",
        "\n",
        "                assert images.shape[1] == model.n_channels, \\\n",
        "                    f'Network has been defined with {model.n_channels} input channels, ' \\\n",
        "                    f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n",
        "                    'the images are loaded correctly.'\n",
        "\n",
        "                images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n",
        "                true_masks = true_masks.to(device=device, dtype=torch.long)\n",
        "\n",
        "                with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n",
        "                    masks_pred = model(images)\n",
        "                    if model.n_classes == 1:\n",
        "                        loss = criterion(masks_pred.squeeze(1), true_masks.float())\n",
        "                        loss += dice_loss(F.sigmoid(masks_pred.squeeze(1)), true_masks.float(), multiclass=False)\n",
        "                    else:\n",
        "                        loss = criterion(masks_pred, true_masks)\n",
        "                        loss += dice_loss(\n",
        "                            F.softmax(masks_pred, dim=1).float(),\n",
        "                            F.one_hot(true_masks, model.n_classes).permute(0, 3, 1, 2).float(),\n",
        "                            multiclass=True\n",
        "                        )\n",
        "\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                grad_scaler.scale(loss).backward()\n",
        "                grad_scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
        "                grad_scaler.step(optimizer)\n",
        "                grad_scaler.update()\n",
        "\n",
        "                pbar.update(images.shape[0])\n",
        "                global_step += 1\n",
        "                epoch_loss += loss.item()\n",
        "                print({\n",
        "                    'train loss': loss.item(),\n",
        "                    'step': global_step,\n",
        "                    'epoch': epoch\n",
        "                })\n",
        "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
        "\n",
        "                # Evaluation round\n",
        "                division_step = (n_train // (5 * batch_size))\n",
        "                if division_step > 0:\n",
        "                    if global_step % division_step == 0:\n",
        "                        histograms = {}\n",
        "                        for tag, value in model.named_parameters():\n",
        "                            if not value.requires_grad:\n",
        "                                continue  # skip frozen layers\n",
        "                            tag = tag.replace('/', '.')\n",
        "                            if not (torch.isinf(value) | torch.isnan(value)).any():\n",
        "                                histograms['Weights/' + tag] = wandb.Histogram(value.data.cpu())\n",
        "                            if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():\n",
        "                                histograms['Gradients/' + tag] = wandb.Histogram(value.grad.data.cpu())\n",
        "\n",
        "                        val_score = evaluate(model, val_loader, device, amp)\n",
        "                        scheduler.step(val_score)\n",
        "\n",
        "                        print('Validation Dice score: {}'.format(val_score))\n",
        "                        try:\n",
        "                            print({\n",
        "                                'learning rate': optimizer.param_groups[0]['lr'],\n",
        "                                'validation Dice': val_score,\n",
        "                                'images': wandb.Image(images[0].cpu()),\n",
        "                                'masks': {\n",
        "                                    'true': wandb.Image(true_masks[0].float().cpu()),\n",
        "                                    'pred': wandb.Image(masks_pred.argmax(dim=1)[0].float().cpu()),\n",
        "                                },\n",
        "                                'step': global_step,\n",
        "                                'epoch': epoch,\n",
        "                                **histograms\n",
        "                            })\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "        if save_checkpoint:\n",
        "            Path(dir_checkpoint).mkdir(parents=True, exist_ok=True)\n",
        "            state_dict = model.state_dict()\n",
        "            torch.save(state_dict, str(dir_checkpoint / 'checkpoint_epoch{}.pth'.format(epoch)))\n",
        "            logging.info(f'Checkpoint {epoch} saved!')\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks')\n",
        "    parser.add_argument('--epochs', '-e', metavar='E', type=int, default=5, help='Number of epochs')\n",
        "    parser.add_argument('--batch-size', '-b', dest='batch_size', metavar='B', type=int, default=1, help='Batch size')\n",
        "    parser.add_argument('--learning-rate', '-l', metavar='LR', type=float, default=1e-5,\n",
        "                        help='Learning rate', dest='lr')\n",
        "    parser.add_argument('--load', '-f', type=str, default=False, help='Load model from a .pth file')\n",
        "    parser.add_argument('--scale', '-s', type=float, default=0.5, help='Downscaling factor of the images')\n",
        "    parser.add_argument('--validation', '-v', dest='val', type=float, default=10.0,\n",
        "                        help='Percent of the data that is used as validation (0-100)')\n",
        "    parser.add_argument('--amp', action='store_true', default=False, help='Use mixed precision')\n",
        "    parser.add_argument('--bilinear', action='store_true', default=False, help='Use bilinear upsampling')\n",
        "    parser.add_argument('--classes', '-c', type=int, default=2, help='Number of classes')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "train_model(model=model, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
      ],
      "id": "f1186f2257f7ed50",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:00<00:00, 194.01it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>train loss</td><td>█▆▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>step</td><td>100</td></tr><tr><td>train loss</td><td>1.21434</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">super-aardvark-2</strong> at: <a href='https://wandb.ai/anony-moose-681487331371034590/U-Net/runs/n3tm15zv?apiKey=333310392d0546ba0c08aa72b8dba7a14c0cff62' target=\"_blank\">https://wandb.ai/anony-moose-681487331371034590/U-Net/runs/n3tm15zv?apiKey=333310392d0546ba0c08aa72b8dba7a14c0cff62</a><br> View project at: <a href='https://wandb.ai/anony-moose-681487331371034590/U-Net?apiKey=333310392d0546ba0c08aa72b8dba7a14c0cff62' target=\"_blank\">https://wandb.ai/anony-moose-681487331371034590/U-Net?apiKey=333310392d0546ba0c08aa72b8dba7a14c0cff62</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250703_073746-n3tm15zv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/Pytorch-UNet/wandb/run-20250703_074235-jy5i6nxc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/anony-moose-681487331371034590/U-Net/runs/jy5i6nxc?apiKey=333310392d0546ba0c08aa72b8dba7a14c0cff62' target=\"_blank\">crisp-darkness-3</a></strong> to <a href='https://wandb.ai/anony-moose-681487331371034590/U-Net?apiKey=333310392d0546ba0c08aa72b8dba7a14c0cff62' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/anony-moose-681487331371034590/U-Net?apiKey=333310392d0546ba0c08aa72b8dba7a14c0cff62' target=\"_blank\">https://wandb.ai/anony-moose-681487331371034590/U-Net?apiKey=333310392d0546ba0c08aa72b8dba7a14c0cff62</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/anony-moose-681487331371034590/U-Net/runs/jy5i6nxc?apiKey=333310392d0546ba0c08aa72b8dba7a14c0cff62' target=\"_blank\">https://wandb.ai/anony-moose-681487331371034590/U-Net/runs/jy5i6nxc?apiKey=333310392d0546ba0c08aa72b8dba7a14c0cff62</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Do NOT share these links with anyone. They can be used to claim your runs."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-14-2498790085.py:78: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training:\n",
            "        Epochs:          100\n",
            "        Batch size:      16\n",
            "        Learning rate:   0.0001\n",
            "        Training size:   13\n",
            "        Validation size: 1\n",
            "        Checkpoints:     True\n",
            "        Device:          cuda\n",
            "        Images scaling:  1\n",
            "        Mixed Precision: True\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/100: 100%|██████████| 13/13 [00:00<00:00, 18.37img/s, loss (batch)=1.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.213775873184204, 'step': 1, 'epoch': 1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/100: 100%|██████████| 13/13 [00:00<00:00, 13.96img/s, loss (batch)=1.27]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2737846374511719, 'step': 2, 'epoch': 2}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/100: 100%|██████████| 13/13 [00:00<00:00, 16.10img/s, loss (batch)=1.25]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2460741996765137, 'step': 3, 'epoch': 3}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/100: 100%|██████████| 13/13 [00:00<00:00, 16.33img/s, loss (batch)=1.23]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2342851161956787, 'step': 4, 'epoch': 4}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/100: 100%|██████████| 13/13 [00:00<00:00, 13.54img/s, loss (batch)=1.23]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2292039394378662, 'step': 5, 'epoch': 5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/100: 100%|██████████| 13/13 [00:00<00:00, 16.72img/s, loss (batch)=1.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2248616218566895, 'step': 6, 'epoch': 6}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/100: 100%|██████████| 13/13 [00:00<00:00, 15.22img/s, loss (batch)=1.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2226529121398926, 'step': 7, 'epoch': 7}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/100: 100%|██████████| 13/13 [00:00<00:00, 20.49img/s, loss (batch)=1.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2222018241882324, 'step': 8, 'epoch': 8}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/100: 100%|██████████| 13/13 [00:01<00:00, 12.06img/s, loss (batch)=1.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.22090482711792, 'step': 9, 'epoch': 9}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/100: 100%|██████████| 13/13 [00:00<00:00, 20.08img/s, loss (batch)=1.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2192370891571045, 'step': 10, 'epoch': 10}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/100: 100%|██████████| 13/13 [00:00<00:00, 15.12img/s, loss (batch)=1.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2175565958023071, 'step': 11, 'epoch': 11}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/100: 100%|██████████| 13/13 [00:00<00:00, 19.80img/s, loss (batch)=1.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2170701026916504, 'step': 12, 'epoch': 12}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/100: 100%|██████████| 13/13 [00:00<00:00, 20.38img/s, loss (batch)=1.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2156976461410522, 'step': 13, 'epoch': 13}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/100: 100%|██████████| 13/13 [00:01<00:00, 11.51img/s, loss (batch)=1.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2152211666107178, 'step': 14, 'epoch': 14}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/100: 100%|██████████| 13/13 [00:00<00:00, 20.08img/s, loss (batch)=1.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.21333909034729, 'step': 15, 'epoch': 15}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/100: 100%|██████████| 13/13 [00:00<00:00, 15.03img/s, loss (batch)=1.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2121819257736206, 'step': 16, 'epoch': 16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/100: 100%|██████████| 13/13 [00:00<00:00, 16.10img/s, loss (batch)=1.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2113851308822632, 'step': 17, 'epoch': 17}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/100: 100%|██████████| 13/13 [00:01<00:00, 12.64img/s, loss (batch)=1.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.209702968597412, 'step': 18, 'epoch': 18}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/100: 100%|██████████| 13/13 [00:00<00:00, 16.49img/s, loss (batch)=1.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2094541788101196, 'step': 19, 'epoch': 19}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/100: 100%|██████████| 13/13 [00:00<00:00, 16.56img/s, loss (batch)=1.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2087948322296143, 'step': 20, 'epoch': 20}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/100: 100%|██████████| 13/13 [00:00<00:00, 15.34img/s, loss (batch)=1.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2076029777526855, 'step': 21, 'epoch': 21}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/100: 100%|██████████| 13/13 [00:00<00:00, 20.01img/s, loss (batch)=1.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.207152247428894, 'step': 22, 'epoch': 22}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/100: 100%|██████████| 13/13 [00:01<00:00, 11.63img/s, loss (batch)=1.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.205730676651001, 'step': 23, 'epoch': 23}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/100: 100%|██████████| 13/13 [00:00<00:00, 20.23img/s, loss (batch)=1.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2052054405212402, 'step': 24, 'epoch': 24}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/100: 100%|██████████| 13/13 [00:01<00:00, 11.47img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.204393744468689, 'step': 25, 'epoch': 25}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/100: 100%|██████████| 13/13 [00:00<00:00, 19.15img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.203679084777832, 'step': 26, 'epoch': 26}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/100: 100%|██████████| 13/13 [00:00<00:00, 18.61img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2028939723968506, 'step': 27, 'epoch': 27}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/100: 100%|██████████| 13/13 [00:00<00:00, 15.72img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2023110389709473, 'step': 28, 'epoch': 28}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/100: 100%|██████████| 13/13 [00:00<00:00, 20.40img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2016314268112183, 'step': 29, 'epoch': 29}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/100: 100%|██████████| 13/13 [00:01<00:00, 11.22img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2009708881378174, 'step': 30, 'epoch': 30}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/100: 100%|██████████| 13/13 [00:00<00:00, 15.64img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.2002850770950317, 'step': 31, 'epoch': 31}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/100: 100%|██████████| 13/13 [00:00<00:00, 13.46img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1996835470199585, 'step': 32, 'epoch': 32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/100: 100%|██████████| 13/13 [00:00<00:00, 17.81img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1990485191345215, 'step': 33, 'epoch': 33}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/100: 100%|██████████| 13/13 [00:01<00:00, 10.75img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1984491348266602, 'step': 34, 'epoch': 34}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/100: 100%|██████████| 13/13 [00:00<00:00, 20.07img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1978880167007446, 'step': 35, 'epoch': 35}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/100: 100%|██████████| 13/13 [00:00<00:00, 20.49img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.19729745388031, 'step': 36, 'epoch': 36}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/100: 100%|██████████| 13/13 [00:01<00:00, 12.18img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1970151662826538, 'step': 37, 'epoch': 37}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/100: 100%|██████████| 13/13 [00:00<00:00, 19.92img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1976628303527832, 'step': 38, 'epoch': 38}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/100: 100%|██████████| 13/13 [00:01<00:00, 11.55img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.196009874343872, 'step': 39, 'epoch': 39}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/100: 100%|██████████| 13/13 [00:00<00:00, 19.72img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1959216594696045, 'step': 40, 'epoch': 40}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/100: 100%|██████████| 13/13 [00:00<00:00, 15.16img/s, loss (batch)=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1950645446777344, 'step': 41, 'epoch': 41}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/100: 100%|██████████| 13/13 [00:00<00:00, 20.37img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1944774389266968, 'step': 42, 'epoch': 42}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/100: 100%|██████████| 13/13 [00:00<00:00, 20.40img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.193937063217163, 'step': 43, 'epoch': 43}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/100: 100%|██████████| 13/13 [00:00<00:00, 14.44img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1933605670928955, 'step': 44, 'epoch': 44}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/100: 100%|██████████| 13/13 [00:00<00:00, 19.77img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1931589841842651, 'step': 45, 'epoch': 45}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/100: 100%|██████████| 13/13 [00:01<00:00, 10.20img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1925019025802612, 'step': 46, 'epoch': 46}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/100: 100%|██████████| 13/13 [00:00<00:00, 16.79img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1921563148498535, 'step': 47, 'epoch': 47}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/100: 100%|██████████| 13/13 [00:00<00:00, 13.36img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1914560794830322, 'step': 48, 'epoch': 48}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/100: 100%|██████████| 13/13 [00:00<00:00, 19.97img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1910514831542969, 'step': 49, 'epoch': 49}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/100: 100%|██████████| 13/13 [00:00<00:00, 20.60img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1905711889266968, 'step': 50, 'epoch': 50}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 51/100: 100%|██████████| 13/13 [00:01<00:00, 11.22img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1897857189178467, 'step': 51, 'epoch': 51}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 52/100: 100%|██████████| 13/13 [00:00<00:00, 20.99img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1894714832305908, 'step': 52, 'epoch': 52}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 53/100: 100%|██████████| 13/13 [00:00<00:00, 15.31img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1888558864593506, 'step': 53, 'epoch': 53}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 54/100: 100%|██████████| 13/13 [00:00<00:00, 19.98img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1882317066192627, 'step': 54, 'epoch': 54}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 55/100: 100%|██████████| 13/13 [00:01<00:00, 11.56img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1878708600997925, 'step': 55, 'epoch': 55}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 56/100: 100%|██████████| 13/13 [00:00<00:00, 20.35img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1870840787887573, 'step': 56, 'epoch': 56}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 57/100: 100%|██████████| 13/13 [00:00<00:00, 20.00img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.186659574508667, 'step': 57, 'epoch': 57}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 58/100: 100%|██████████| 13/13 [00:00<00:00, 13.55img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1860512495040894, 'step': 58, 'epoch': 58}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 59/100: 100%|██████████| 13/13 [00:00<00:00, 20.88img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1854764223098755, 'step': 59, 'epoch': 59}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 60/100: 100%|██████████| 13/13 [00:01<00:00, 10.17img/s, loss (batch)=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1850826740264893, 'step': 60, 'epoch': 60}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 61/100: 100%|██████████| 13/13 [00:00<00:00, 16.57img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1843533515930176, 'step': 61, 'epoch': 61}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 62/100: 100%|██████████| 13/13 [00:00<00:00, 17.69img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.184108018875122, 'step': 62, 'epoch': 62}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 63/100: 100%|██████████| 13/13 [00:00<00:00, 14.78img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1833754777908325, 'step': 63, 'epoch': 63}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 64/100: 100%|██████████| 13/13 [00:00<00:00, 21.71img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1828774213790894, 'step': 64, 'epoch': 64}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 65/100: 100%|██████████| 13/13 [00:01<00:00, 11.71img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1823856830596924, 'step': 65, 'epoch': 65}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 66/100: 100%|██████████| 13/13 [00:00<00:00, 21.32img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1823554039001465, 'step': 66, 'epoch': 66}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 67/100: 100%|██████████| 13/13 [00:01<00:00, 11.40img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1819257736206055, 'step': 67, 'epoch': 67}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 68/100: 100%|██████████| 13/13 [00:00<00:00, 20.72img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1808892488479614, 'step': 68, 'epoch': 68}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 69/100: 100%|██████████| 13/13 [00:00<00:00, 20.59img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1806352138519287, 'step': 69, 'epoch': 69}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 70/100: 100%|██████████| 13/13 [00:01<00:00, 11.79img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1799941062927246, 'step': 70, 'epoch': 70}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 71/100: 100%|██████████| 13/13 [00:00<00:00, 20.19img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1795796155929565, 'step': 71, 'epoch': 71}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 72/100: 100%|██████████| 13/13 [00:00<00:00, 15.26img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1791661977767944, 'step': 72, 'epoch': 72}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 73/100: 100%|██████████| 13/13 [00:00<00:00, 20.14img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1788196563720703, 'step': 73, 'epoch': 73}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 74/100: 100%|██████████| 13/13 [00:00<00:00, 13.06img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1783194541931152, 'step': 74, 'epoch': 74}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 75/100: 100%|██████████| 13/13 [00:00<00:00, 16.68img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1779842376708984, 'step': 75, 'epoch': 75}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 76/100: 100%|██████████| 13/13 [00:00<00:00, 17.23img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.177419900894165, 'step': 76, 'epoch': 76}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 77/100: 100%|██████████| 13/13 [00:01<00:00, 10.69img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1767451763153076, 'step': 77, 'epoch': 77}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 78/100: 100%|██████████| 13/13 [00:00<00:00, 20.77img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1763818264007568, 'step': 78, 'epoch': 78}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 79/100: 100%|██████████| 13/13 [00:00<00:00, 18.00img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1756930351257324, 'step': 79, 'epoch': 79}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 80/100: 100%|██████████| 13/13 [00:00<00:00, 20.68img/s, loss (batch)=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1753238439559937, 'step': 80, 'epoch': 80}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 81/100: 100%|██████████| 13/13 [00:01<00:00, 10.79img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1747121810913086, 'step': 81, 'epoch': 81}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 82/100: 100%|██████████| 13/13 [00:00<00:00, 20.34img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1743665933609009, 'step': 82, 'epoch': 82}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 83/100: 100%|██████████| 13/13 [00:00<00:00, 17.77img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1739287376403809, 'step': 83, 'epoch': 83}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 84/100: 100%|██████████| 13/13 [00:00<00:00, 14.98img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1734867095947266, 'step': 84, 'epoch': 84}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 85/100: 100%|██████████| 13/13 [00:00<00:00, 20.40img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.173239827156067, 'step': 85, 'epoch': 85}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 86/100: 100%|██████████| 13/13 [00:01<00:00, 11.60img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.172558069229126, 'step': 86, 'epoch': 86}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 87/100: 100%|██████████| 13/13 [00:00<00:00, 19.45img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1722040176391602, 'step': 87, 'epoch': 87}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 88/100: 100%|██████████| 13/13 [00:00<00:00, 13.51img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.171419620513916, 'step': 88, 'epoch': 88}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 89/100: 100%|██████████| 13/13 [00:00<00:00, 16.42img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.171275019645691, 'step': 89, 'epoch': 89}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 90/100: 100%|██████████| 13/13 [00:00<00:00, 16.97img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.170761227607727, 'step': 90, 'epoch': 90}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 91/100: 100%|██████████| 13/13 [00:01<00:00, 10.94img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1706258058547974, 'step': 91, 'epoch': 91}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 92/100: 100%|██████████| 13/13 [00:00<00:00, 20.79img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1700499057769775, 'step': 92, 'epoch': 92}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 93/100: 100%|██████████| 13/13 [00:01<00:00, 11.49img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1696571111679077, 'step': 93, 'epoch': 93}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 94/100: 100%|██████████| 13/13 [00:00<00:00, 21.14img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1690516471862793, 'step': 94, 'epoch': 94}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 95/100: 100%|██████████| 13/13 [00:01<00:00, 11.47img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1685667037963867, 'step': 95, 'epoch': 95}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 96/100: 100%|██████████| 13/13 [00:00<00:00, 20.57img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.168075442314148, 'step': 96, 'epoch': 96}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 97/100: 100%|██████████| 13/13 [00:00<00:00, 20.03img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1675359010696411, 'step': 97, 'epoch': 97}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 98/100: 100%|██████████| 13/13 [00:01<00:00, 11.64img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.16700279712677, 'step': 98, 'epoch': 98}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 99/100: 100%|██████████| 13/13 [00:00<00:00, 20.39img/s, loss (batch)=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1667426824569702, 'step': 99, 'epoch': 99}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 100/100: 100%|██████████| 13/13 [00:01<00:00, 11.59img/s, loss (batch)=1.17]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train loss': 1.1662018299102783, 'step': 100, 'epoch': 100}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj13UBGGP6HV",
        "outputId": "813b2d34-350d-4022-87df-b8666160cfc2"
      },
      "id": "Aj13UBGGP6HV",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/drive/MyDrive/Brick_Data_Train.zip'  # replace with your actual path\n",
        "extract_to = '/content/drive/MyDrive/Brick_Data_Train_unzipped'  # or wherever you want\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)"
      ],
      "metadata": {
        "id": "xrU48zlCRjaO"
      },
      "id": "xrU48zlCRjaO",
      "execution_count": 2,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}